{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPMq2VCjzIt2/nSVpAlve9a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZuXlfoZhaBx","executionInfo":{"status":"ok","timestamp":1765025055568,"user_tz":-330,"elapsed":1622,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"a5cce142-f561-40da-8418-cc6cf9c98747"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import ast\n","import re\n","import spacy\n","\n","DATA_PATH = \"/content/drive/MyDrive/news_project/data/raw/news.tsv\"\n","df = pd.read_csv(DATA_PATH,sep=\"\\t\")\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","df[\"Headline\"] = df[\"Headline\"].astype(str)\n","df[\"Title entity\"] = df[\"Title entity\"].astype(str)\n","\n","COUNTRIES = [\"United States\", \"India\", \"Brazil\", \"China\", \"Mexico\", \"Canada\"]\n","PERSON_PATTERN = r\"^[A-Z][a-z]+(\\s[A-Z][a-z]+)+$\"\n","ORG_KEYWORDS = [\"Corporation\", \"Authority\", \"Committee\", \"Association\", \"University\", \"Agency\", \"Company\", \"FC\", \"Ltd\"]\n","\n","\n","def infer_entity_type(expanded):\n","    expanded = expanded.strip()\n","\n","    if re.match(PERSON_PATTERN, expanded):\n","        return \"PERSON\"\n","\n","    if expanded in COUNTRIES:\n","        return \"LOCATION\"\n","\n","    if any(k in expanded for k in ORG_KEYWORDS):\n","        return \"ORG\"\n","\n","    return \"MISC\"\n","\n","def convert_to_bio(text, entity_string):\n","    tokens = text.split()\n","    tags = [\"O\"] * len(tokens)\n","\n","    if entity_string == \"{}\":\n","        return tokens, tags\n","\n","    try:\n","        ent_dict = ast.literal_eval(entity_string)\n","    except:\n","        return tokens, tags\n","\n","    lower_tokens = [w.lower().strip(\".,!?\") for w in tokens]\n","\n","    for surface, expanded in ent_dict.items():\n","        clean_surface = surface.replace(\"'s\", \"\").strip()\n","        stoks = clean_surface.split()\n","        stoks = [w.lower().strip(\".,!?\") for w in stoks]\n","        n = len(stoks)\n","\n","        ent_type = infer_entity_type(expanded)\n","\n","        # Search entity span safely\n","        for i in range(len(tokens)):\n","            try:\n","                if lower_tokens[i:i+n] == stoks:\n","                    tags[i] = f\"B-{ent_type}\"\n","                    for j in range(i+1, i+n):\n","                        if j < len(tags):   # SAFETY CHECK\n","                            tags[j] = f\"I-{ent_type}\"\n","            except:\n","                continue\n","\n","    return tokens, tags\n","\n","\n","\n","sentences = []\n","labels = []\n","\n","for _, row in df.iterrows():\n","    s, t = convert_to_bio(row[\"Headline\"], row[\"Title entity\"])\n","    sentences.append(s)\n","    labels.append(t)\n","\n","print(\"DATA READY — Samples:\", len(sentences))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcTrlbW1jYZe","executionInfo":{"status":"ok","timestamp":1765025126974,"user_tz":-330,"elapsed":71410,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"8588424c-4e43-4885-b9f8-41e8779fdd56"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["DATA READY — Samples: 113762\n"]}]},{"cell_type":"code","source":["!pip install keras tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9oVtQNA5skxa","executionInfo":{"status":"ok","timestamp":1765025209682,"user_tz":-330,"elapsed":58002,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"bc958a50-0197-4ae7-a01d-d8c7076b2fdc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras\n","  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n","Collecting tensorflow\n","  Using cached tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n","Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n","Collecting tensorboard~=2.20.0 (from tensorflow)\n","  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n","Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.19.0\n","    Uninstalling tensorboard-2.19.0:\n","      Successfully uninstalled tensorboard-2.19.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n","tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n","tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-3.12.0 tensorboard-2.20.0 tensorflow-2.20.0\n"]}]},{"cell_type":"code","source":["# ENCODING & VOCAB GENERATION\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","word_set = set(w for s in sentences for w in s)\n","tag_set = set(t for seq in labels for t in seq)\n","\n","word2idx = {w: i+2 for i, w in enumerate(sorted(word_set))}\n","word2idx[\"<PAD>\"] = 0\n","word2idx[\"<OOV>\"] = 1\n","\n","tag2idx = {t: i for i, t in enumerate(sorted(tag_set))}\n","idx2tag = {v: k for k, v in tag2idx.items()}\n","\n","MAX_LEN = 60\n","\n","X = [[word2idx.get(w, 1) for w in seq] for seq in sentences]\n","X = pad_sequences(X, maxlen=MAX_LEN, padding=\"post\")\n","\n","y = [[tag2idx[t] for t in seq] for seq in labels]\n","y = pad_sequences(y, maxlen=MAX_LEN, padding=\"post\")\n","y_cat = to_categorical(y, num_classes=len(tag2idx))\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, y_cat, test_size=0.2, random_state=42\n",")\n","\n","VOCAB_SIZE = len(word2idx)\n","NUM_TAGS = len(tag2idx)\n","\n","print(\"VOCAB SIZE:\", VOCAB_SIZE)\n","print(\"NUM TAGS:\", NUM_TAGS)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZQyNS_hmLKo","executionInfo":{"status":"ok","timestamp":1765025235129,"user_tz":-330,"elapsed":20578,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"febb7687-9acd-4259-bc70-1d5c492f8985"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["VOCAB SIZE: 104705\n","NUM TAGS: 8\n"]}]},{"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1bw1vMcImpJU","executionInfo":{"status":"ok","timestamp":1765025488652,"user_tz":-330,"elapsed":246579,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"18a0256d-9dc2-415e-b761-bcf8e57c7691"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-06 12:47:22--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2025-12-06 12:47:22--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2025-12-06 12:47:22--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip.1’\n","\n","glove.6B.zip.1      100%[===================>] 822.24M  5.08MB/s    in 2m 39s  \n","\n","2025-12-06 12:50:02 (5.16 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: glove.6B.50d.txt        \n","replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: glove.6B.100d.txt       \n","replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: glove.6B.200d.txt       \n","replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: glove.6B.300d.txt       \n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Azju2qawoJ8y","executionInfo":{"status":"ok","timestamp":1765025502443,"user_tz":-330,"elapsed":5794,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"9461e981-bdf0-47b0-df5c-aa2ed058320c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"]}]},{"cell_type":"code","source":["# BUILD EMBEDDINGS\n","\n","#  Word2vec\n","\n","import numpy as np\n","from gensim.models import Word2Vec\n","w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n","\n","embedding_w2v = np.zeros((VOCAB_SIZE, 100))\n","for w, i in word2idx.items():\n","    embedding_w2v[i] = w2v_model.wv[w] if w in w2v_model.wv else np.random.normal(0,0.6,100)\n"],"metadata":{"id":"0f5fz6pUmNUI","executionInfo":{"status":"ok","timestamp":1765025536549,"user_tz":-330,"elapsed":30674,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# GloVe\n","\n","GLOVE_PATH = \"/content/glove.6B.100d.txt\"\n","import numpy as np\n","\n","glove_index = {}\n","with open(GLOVE_PATH, encoding=\"utf8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.asarray(values[1:], dtype=\"float32\")\n","        glove_index[word] = vector\n","\n","print(\"Total embeddings found:\", len(glove_index))\n","\n","# Matrix\n","\n","embedding_glove = np.zeros((VOCAB_SIZE, 100))\n","\n","for w, i in word2idx.items():\n","    embedding_glove[i] = glove_index.get(\n","        w, np.random.normal(scale=0.6, size=(100,))\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRpxcQUOoBEM","executionInfo":{"status":"ok","timestamp":1765025562354,"user_tz":-330,"elapsed":18005,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"e6b36692-709e-4520-fe31-85fd13486a7d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Total embeddings found: 400000\n"]}]},{"cell_type":"code","source":["# !pip uninstall -y tensorflow keras\n","# !pip install tensorflow==2.11 keras==2.11 tensorflow-addons==0.20.0\n","# !pip install keras-crf\n"],"metadata":{"id":"IP7AO-uVpWZC","executionInfo":{"status":"aborted","timestamp":1765025127123,"user_tz":-330,"elapsed":73359,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install tensorflow-addons==0.22.0\n"],"metadata":{"id":"BjpNDmAzrCB-","executionInfo":{"status":"aborted","timestamp":1765025127124,"user_tz":-330,"elapsed":73355,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n","\n","# ============================================\n","# Regular BiLSTM Model\n","# ============================================\n","\n","def build_bilstm(embedding_matrix):\n","    inp = Input(shape=(MAX_LEN,))\n","    emb = Embedding(\n","        VOCAB_SIZE, 100,\n","        weights=[embedding_matrix],\n","        mask_zero=True,\n","        trainable=False\n","    )(inp)\n","\n","    x = Bidirectional(LSTM(128, return_sequences=True))(emb)\n","    out = TimeDistributed(Dense(NUM_TAGS, activation=\"softmax\"))(x)\n","\n","    model = Model(inp, out)\n","    model.compile(\n","        optimizer=\"adam\",\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"accuracy\"]\n","    )\n","    return model\n","\n","\n","# ============================================\n","# BiLSTM + CRF-like loss\n","# (NO tensorflow_addons needed)\n","# ============================================\n","\n","def build_bilstm_crf(embedding_matrix):\n","    inp = Input(shape=(MAX_LEN,))\n","    emb = Embedding(\n","        VOCAB_SIZE, 100,\n","        weights=[embedding_matrix],\n","        mask_zero=True,\n","        trainable=False\n","    )(inp)\n","\n","    x = Bidirectional(LSTM(128, return_sequences=True))(emb)\n","    logits = TimeDistributed(Dense(NUM_TAGS))(x)\n","\n","    # Learnable CRF transition matrix\n","    transitions = tf.Variable(\n","        tf.random.uniform(shape=(NUM_TAGS, NUM_TAGS)),\n","        name=\"transition_matrix\"\n","    )\n","\n","    def crf_loss(y_true, y_pred):\n","        \"\"\"\n","        y_true → one-hot target\n","        y_pred → logits\n","        \"\"\"\n","        y_true_idx = tf.argmax(y_true, axis=-1)  # (batch, seq)\n","\n","        # emission log probabilities\n","        log_softmax = tf.nn.log_softmax(y_pred, axis=-1)\n","\n","        # likelihood of correct token prediction\n","        token_ll = tf.reduce_sum(\n","            tf.reduce_sum(\n","                tf.one_hot(y_true_idx, NUM_TAGS) * log_softmax,\n","                axis=-1\n","            ),\n","            axis=-1\n","        )\n","\n","        # transition score\n","        seq_score = 0.0\n","\n","        for t in range(MAX_LEN - 1):\n","            curr = y_true_idx[:, t]\n","            nxt = y_true_idx[:, t + 1]\n","\n","            seq_score += tf.gather_nd(\n","                transitions,\n","                tf.stack([curr, nxt], axis=1)\n","            )\n","\n","        loss = -(token_ll + seq_score)\n","\n","        return tf.reduce_mean(loss)\n","\n","    model = Model(inp, logits)\n","    model.compile(optimizer=\"adam\", loss=crf_loss)\n","\n","    return model\n"],"metadata":{"id":"l913_IUQpHOv","executionInfo":{"status":"ok","timestamp":1765025646594,"user_tz":-330,"elapsed":54,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["results = []\n","models_dict = {}\n","\n","print(\"\\nTraining MODEL-1: BiLSTM + Word2Vec\")\n","model_1 = build_bilstm(embedding_w2v)\n","model_1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n","models_dict[\"1\"] = model_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HabNcDCVumtu","executionInfo":{"status":"ok","timestamp":1765028024855,"user_tz":-330,"elapsed":135070,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"6502e53a-8bea-4bae-f412-0925d344a309"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training MODEL-1: BiLSTM + Word2Vec\n","Epoch 1/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 21ms/step - accuracy: 0.9161 - loss: 0.2971 - val_accuracy: 0.9205 - val_loss: 0.2663\n","Epoch 2/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 24ms/step - accuracy: 0.9242 - loss: 0.2495 - val_accuracy: 0.9251 - val_loss: 0.2475\n"]}]},{"cell_type":"code","source":["\n","print(\"\\nTraining MODEL-2: BiLSTM + GloVe\")\n","model_2 = build_bilstm(embedding_glove)\n","model_2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n","models_dict[\"2\"] = model_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIhOE3kHvZ-q","executionInfo":{"status":"ok","timestamp":1765026295441,"user_tz":-330,"elapsed":301951,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"1f622e6f-05dd-4653-c66a-4af8d032fbe7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training MODEL-2: BiLSTM + GloVe\n","Epoch 1/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 20ms/step - accuracy: 0.9159 - loss: 0.3110 - val_accuracy: 0.9251 - val_loss: 0.2510\n","Epoch 2/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 20ms/step - accuracy: 0.9349 - loss: 0.2131 - val_accuracy: 0.9363 - val_loss: 0.2084\n","Epoch 3/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 20ms/step - accuracy: 0.9440 - loss: 0.1773 - val_accuracy: 0.9394 - val_loss: 0.1966\n","Epoch 4/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 20ms/step - accuracy: 0.9507 - loss: 0.1525 - val_accuracy: 0.9413 - val_loss: 0.1921\n","Epoch 5/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 25ms/step - accuracy: 0.9565 - loss: 0.1323 - val_accuracy: 0.9411 - val_loss: 0.1937\n"]}]},{"cell_type":"code","source":["print(\"\\nTraining MODEL-3: BiLSTM-CRF + Word2Vec\")\n","model_3 = build_bilstm_crf(embedding_w2v)\n","model_3.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n","models_dict[\"3\"] = model_3\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tJZoQpuw6y1","executionInfo":{"status":"ok","timestamp":1765026604991,"user_tz":-330,"elapsed":307093,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"babc9095-7517-4951-c170-c9faba864dd6"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training MODEL-3: BiLSTM-CRF + Word2Vec\n","Epoch 1/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 20ms/step - loss: 25.8660 - val_loss: -2.2927\n","Epoch 2/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 18ms/step - loss: -6.2627 - val_loss: -8.2341\n","Epoch 3/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 19ms/step - loss: -8.9154 - val_loss: -9.2397\n","Epoch 4/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 19ms/step - loss: -9.4977 - val_loss: -9.5105\n","Epoch 5/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 19ms/step - loss: -9.7082 - val_loss: -9.5911\n"]}]},{"cell_type":"code","source":["print(\"\\nTraining MODEL-4: BiLSTM-CRF + GloVe\")\n","model_4 = build_bilstm_crf(embedding_glove)\n","model_4.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n","models_dict[\"4\"] = model_4\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"funkp1iZpQbj","executionInfo":{"status":"ok","timestamp":1765026734216,"user_tz":-330,"elapsed":121489,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"43f18481-3903-432b-c3f3-ad20a9c76be6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training MODEL-4: BiLSTM-CRF + GloVe\n","Epoch 1/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 21ms/step - loss: 33.4681 - val_loss: 4.9757\n","Epoch 2/5\n","\u001b[1m2845/2845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 19ms/step - loss: 0.8262 - val_loss: -1.1629\n"]}]},{"cell_type":"code","source":["# Create folder before saving\n","!mkdir -p \"/content/drive/MyDrive/news_project/NER_eval\"\n","\n","from sklearn.metrics import classification_report\n","\n","def evaluate(model, model_name, embed_name):\n","    preds = model.predict(X_val).argmax(axis=-1)\n","    true_tags = y_val.argmax(axis=-1)\n","\n","    y_true = []\n","    y_pred = []\n","\n","    for t_seq, p_seq in zip(true_tags, preds):\n","        for t, p in zip(t_seq, p_seq):\n","            if idx2tag[t] != \"O\":\n","                y_true.append(idx2tag[t])\n","                y_pred.append(idx2tag[p])\n","\n","    report = classification_report(\n","        y_true,\n","        y_pred,\n","        output_dict=True,\n","        zero_division=0\n","    )\n","\n","    weighted = report[\"weighted avg\"]\n","\n","    return {\n","        \"Model\": model_name,\n","        \"Embedding\": embed_name,\n","        \"Precision\": weighted[\"precision\"],\n","        \"Recall\": weighted[\"recall\"],\n","        \"F1 Score\": weighted[\"f1-score\"]\n","    }\n","\n","results.append(evaluate(model_1, \"BiLSTM\", \"Word2Vec\"))\n","results.append(evaluate(model_2, \"BiLSTM\", \"GloVe\"))\n","results.append(evaluate(model_3, \"BiLSTM-CRF\", \"Word2Vec\"))\n","results.append(evaluate(model_4, \"BiLSTM-CRF\", \"GloVe\"))\n","\n","df_result = pd.DataFrame(results)\n","df_result.to_csv(\"/content/drive/MyDrive/news_project/NER_eval/model_comparison_NER.csv\", index=False)\n","print(\"Evaluation Completed and File Saved\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DewuAqEvzeEu","executionInfo":{"status":"ok","timestamp":1765028106874,"user_tz":-330,"elapsed":72649,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"836ba829-78a8-4a79-cdac-350086cf1dd3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m712/712\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step\n","\u001b[1m712/712\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step\n","\u001b[1m712/712\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n","\u001b[1m712/712\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n","Evaluation Completed and File Saved\n"]}]},{"cell_type":"code","source":["best_row = df_result.loc[df_result[\"F1 Score\"].idxmax()]\n","print(\"\\nBEST MODEL SELECTED:\\n\", best_row)\n","\n","best_tuple = (best_row[\"Model\"], best_row[\"Embedding\"])\n","\n","best_model = {\n","    (\"BiLSTM\", \"Word2Vec\"): model_1,\n","    (\"BiLSTM\", \"GloVe\"): model_2,\n","    (\"BiLSTM-CRF\", \"Word2Vec\"): model_3,\n","    (\"BiLSTM-CRF\", \"GloVe\"): model_4,\n","}[best_tuple]\n","\n","best_model.save(\"/content/drive/MyDrive/news_project/NER_eval/final_best_model.keras\")\n","print(\"\\nSAVED BEST MODEL!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lt2L7AO12eBF","executionInfo":{"status":"ok","timestamp":1765028347917,"user_tz":-330,"elapsed":921,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"62118193-ec63-4360-83b2-b291256c5076"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","BEST MODEL SELECTED:\n"," Model        BiLSTM-CRF\n","Embedding      Word2Vec\n","Precision      0.998365\n","Recall         0.987914\n","F1 Score        0.99054\n","Name: 2, dtype: object\n","\n","SAVED BEST MODEL!\n"]}]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"/content/drive/MyDrive/news_project/NER_eval/word2idx.pkl\", \"wb\") as f:\n","    pickle.dump(word2idx, f)\n"],"metadata":{"id":"6ttEwexB41hV","executionInfo":{"status":"ok","timestamp":1765028636711,"user_tz":-330,"elapsed":54,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/news_project/NER_eval/idx2tag.pkl\", \"wb\") as f:\n","    pickle.dump(idx2tag, f)\n"],"metadata":{"id":"RVeM62Cf5o7C","executionInfo":{"status":"ok","timestamp":1765028648367,"user_tz":-330,"elapsed":60,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/news_project/NER_eval/max_len.txt\", \"w\") as f:\n","    f.write(str(MAX_LEN))\n"],"metadata":{"id":"1QhAr8n_597T","executionInfo":{"status":"ok","timestamp":1765028667828,"user_tz":-330,"elapsed":5,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["Testing"],"metadata":{"id":"i3oQ7fKk6Jvn"}},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load model\n","model = load_model(\"/content/drive/MyDrive/news_project/NER_eval/final_best_model.keras\", compile=False)\n","print(\"Model Loaded Successfully\")\n","\n","# Load vocabulary mappings\n","with open(\"/content/drive/MyDrive/news_project/NER_eval/word2idx.pkl\", \"rb\") as f:\n","    word2idx = pickle.load(f)\n","\n","with open(\"/content/drive/MyDrive/news_project/NER_eval/idx2tag.pkl\", \"rb\") as f:\n","    idx2tag = pickle.load(f)\n","\n","# Load max sequence length\n","with open(\"/content/drive/MyDrive/news_project/NER_eval/max_len.txt\") as f:\n","    MAX_LEN = int(f.read())\n","\n","print(\"Vocab & config loaded\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhisV5kx6CAn","executionInfo":{"status":"ok","timestamp":1765028850206,"user_tz":-330,"elapsed":728,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"4dcc8eca-8afb-469c-ab41-a7ab84b9a454"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Loaded Successfully\n","Vocab & config loaded\n"]}]},{"cell_type":"code","source":["def predict_ner(text):\n","    tokens = text.split()\n","\n","    encoded = [word2idx.get(word, word2idx[\"<OOV>\"]) for word in tokens]\n","\n","    padded = pad_sequences([encoded], maxlen=MAX_LEN, padding=\"post\")\n","\n","    pred = model.predict(padded)[0].argmax(axis=-1)\n","\n","    tags = [idx2tag[idx] for idx in pred][:len(tokens)]\n","\n","    return list(zip(tokens, tags))\n"],"metadata":{"id":"rpLkW5006y3f","executionInfo":{"status":"ok","timestamp":1765028869928,"user_tz":-330,"elapsed":45,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["print(predict_ner(\"Apple invested 3 billion dollars in India\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpnhTt4u6zXP","executionInfo":{"status":"ok","timestamp":1765028875022,"user_tz":-330,"elapsed":1207,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"a8ba41c7-a6cc-4215-ac19-f75ed8613b8d"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","[('Apple', 'O'), ('invested', 'O'), ('3', 'O'), ('billion', 'O'), ('dollars', 'O'), ('in', 'O'), ('India', 'B-MISC')]\n"]}]},{"cell_type":"code","source":["def extract_entities(tokens_with_tags):\n","    entities = {}\n","    current_entity = \"\"\n","    current_tag = \"\"\n","\n","    for token, tag in tokens_with_tags:\n","        if tag.startswith(\"B-\"):\n","            ent_type = tag[2:]\n","            if current_entity:\n","                entities.setdefault(current_tag, []).append(current_entity)\n","\n","            current_entity = token\n","            current_tag = ent_type\n","\n","        elif tag.startswith(\"I-\") and current_tag == tag[2:]:\n","            current_entity += \" \" + token\n","\n","        else:\n","            if current_entity:\n","                entities.setdefault(current_tag, []).append(current_entity)\n","\n","            current_entity = \"\"\n","            current_tag = \"\"\n","\n","    if current_entity:\n","        entities.setdefault(current_tag, []).append(current_entity)\n","\n","    return entities\n"],"metadata":{"id":"ce7t0vOy60Tg","executionInfo":{"status":"ok","timestamp":1765028912215,"user_tz":-330,"elapsed":6,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["text = \"Apple invested 3 billion dollars in India and met Joe Biden in Washington\"\n","print(\"INPUT TEXT:\", text)\n","\n","token_tags = predict_ner(text)\n","\n","print(\"\\nToken Prediction:\")\n","for tok, tag in token_tags:\n","    print(f\"{tok:12} --> {tag}\")\n","\n","final_entities = extract_entities(token_tags)\n","\n","print(\"\\nExtracted Entities:\")\n","print(final_entities)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iywtcrpt69sl","executionInfo":{"status":"ok","timestamp":1765028921673,"user_tz":-330,"elapsed":291,"user":{"displayName":"guviproject1","userId":"11001385083961087091"}},"outputId":"052681d7-85c5-4c4e-b53d-2b99bd107ef2"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["INPUT TEXT: Apple invested 3 billion dollars in India and met Joe Biden in Washington\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n","\n","Token Prediction:\n","Apple        --> O\n","invested     --> O\n","3            --> O\n","billion      --> O\n","dollars      --> O\n","in           --> O\n","India        --> O\n","and          --> O\n","met          --> O\n","Joe          --> B-PERSON\n","Biden        --> I-PERSON\n","in           --> O\n","Washington   --> B-MISC\n","\n","Extracted Entities:\n","{'PERSON': ['Joe Biden'], 'MISC': ['Washington']}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z8X85T2y6_6d"},"execution_count":null,"outputs":[]}]}